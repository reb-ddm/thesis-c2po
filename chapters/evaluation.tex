\chapter{Evaluation}\label{chapter:evaluation}


The \cpo\ analysis was implemented in the \goblint\ static analyzer for C programs, which is based on abstract interpretation.
\goblint\ already contains a set of basic abstract domains and analyses,
including interval analysis, inter-procedural analyses, and a non-relational pointer analysis.
The group of these basic analyzes is denoted as \base.
An additional \goblint\ analysis, called \vareq, tracks must-equalities between general expressions
and thus can handle a subset of the behavior of \cpo.
We evaluated the precision gain and the performance loss of the \cpo\ analysis and compared it to \base\ and \vareq.
Moreover, we analyzed the difference in precision and performance between four different
configurations, which differ in the choice of the join algorithm and the choice of the equal function.
The four configurations are denoted as:
\begin{enumerate}
    \item \cpou\, which uses the precise join with the automaton and computes a canonical normal form for each domain element in order to compare them.
    \item \cpod\, which uses the join that considers the partition and computes \emph{equal} again by using the normal form.
    \item \cpot\, which uses the precise join with the automaton and compares the equivalence classes of the partitions.
    \item \cpoq\, which uses the join that considers the partition and computes \emph{equal} by comparing the equivalence classes.
\end{enumerate}
All benchmarks were conducted on a machine with two Intel Xeon Platinum 8260 CPUs and 512 GB of RAM.

\section{Precision}

The precision of the \cpo\ analysis does not depend on the choice of the algorithm for \emph{equal}, as the two possible implementations are functionally equivalent.
Therefore, in the following, we will only compare the precision of the \cpou, \cpod, \base, and \vareq\ analyses.
They were evaluated on two test suites.

The first suite consists of 29 \emph{litmus} tests inspired by real-world programming patterns,
designed to incorporate complex manipulations of pointers.
Each test was annotated with assertions to be verified.
\cpou\ could prove all assertions,
while only 16\% of the assertions could be proven by \base\ and 23\% by \vareq.
\cpod\ proved all assertions except one, which was the \cref{ex:join-cycle}.
These programs may serve as benchmarks for future analyzers.

We further evaluated the precision of \cpo\ on 11 GNU core utilities programs (Coreutils).
We used \cpou\ to identify program invariants and automatically generate assertions.
For one of these programs, the generation of invariants timed out after one hour.
For the remaining nine programs, the \base\ analysis could prove only 24\% of the generated invariants
and \vareq\ 42\%.
In contrast, \cpou\ was able to prove all invariants.
\cpod\ could prove more than 99\% of the invariants.

Table~\ref{tab:summary} summarizes these results.

\begin{table}[t]
    \centering
    \caption[Results of experiments on litmus tests and Coreutils.]{Summary of precision experiments. For each group of programs, the number of programs, the lines of code, and the total number of invariants generated by \cpou\ are given.
        \ding{51} indicates that all assertions are proven. Otherwise, the number of proven assertions is given.}\label{tab:summary}
    % must use \input instead of \include to compile
    \input{content/eval-table}
\end{table}

We can conclude that the \cpo\ analysis can prove many assertions about relations between pointers that are out of reach for the \base\ analysis, and it is also more precise than the \vareq\ analysis.
As expected, the \cpou\ analysis is more precise than the \cpod\ analysis, as its join operation maintains more equalities in case the quantitative automaton contains cycles.
However, the difference in precision is minimal, indicating that in practice, the cases where there is a cycle in the automaton do not occur frequently.
Therefore, the choice of the join operation does not significantly affect the precision of the analysis.

\section{Efficiency}

For evaluating the performance, we executed \cpo\ on the reachability set
of the SV-COMP 2024 benchmarks~\cite{Beyer24} and compared the performance of the four possible configurations against \base.
Each task was given a timeout of 900 seconds and a memory limit of 15 GB.\@
We used the configuration of the analyzer for SV-COMP but removed the \vareq\ analysis.
We ran the analyzer with each of the four configurations and once without \cpo.

\begin{table}[t]
    \centering
    \caption[Results of experiments on SV-COMP.]{Summary of efficiency experiments on the reachability set
        of the SV-COMP 2024 benchmarks.
        All data is measured with respect to the \base\ analysis.
        The total amount of tasks was 15015.
        The table shows the median and 95\% slowdown of each analysis with respect to \base.
        Additionally, the number of tasks that are unreachable for \base, but are proven correct by the analysis is given, as well as the number of tasks that base was able to prove, but which ran into a timeout or out-of-memory error for \cpo.
        }\label{tab:summary-svcomp}
    % must use \input instead of \include to compile
    \input{content/eval-table-2}
\end{table}

We observed that the analysis with \cpo\ has a 95th percentile slowdown of at most a factor 2,8 compared to the base setting,
indicating that 95\% of tests in the benchmark take at most around three times as long as the base setup.
The median slowdown is 1.07-1.08.

The runtime difference of the two configurations varied significantly between the benchmarks.
In some cases, \base\ timed out while \cpo\ proved the property in less than 30 seconds.
However, in most cases, \cpo\ incurred a noticeable performance overhead compared to \base.
Given the wide variance in performance, the mean slowdown is not a significant indicator.

The results suggest that the more precise join operation (used by \cpou\ and \cpot) is slightly less efficient than the partition-join, as predicted.
Moreover, the normal form algorithm (used by \cpou\ and \cpod) is not more efficient than the other algorithm for \emph{equal}, contrary to our expectations.
However, the difference in performance between the four configurations is not significantly large.
In total, the results demonstrate that \cpo\ scales reasonably well and that the four possible configurations are mostly equivalent in terms of efficiency.

The reachability set of the SV-COMP benchmarks contains over 20,000 tests where the reachability of an error function is to be decided.
The analysis with \cpo\ infers extra properties, which do not necessarily improve the verdict,
as finding the correct verdict may require intricate arithmetic reasoning, which is orthogonal to our goal here.
For this reason, our analysis only succeeds in proving the verdict for 20 SV-COMP tests that \base\ could not prove.
They are the same 20 passed tests for each of the four configurations.
As expected, those 20 tests contain complicated pointer or struct manipulations.
Due to the computational overhead, approximately 80 of the tests that could be proven by \base\ timed out with \cpo\ out of more than 2000 passed verdicts.

The results are summarized in \cref{tab:summary-svcomp}.

Given the negligible differences observed across the four configurations, it is reasonable to conclude that these configurations are equivalent in practice in terms of performance and precision.
Therefore, the most practical choice may be the most straightforward configuration to implement---specifically, the one that avoids computing the normal form and employs the less precise join operation (\cpoq).

The implementation is publicly available under\dots \todo{put on zenodo?}
