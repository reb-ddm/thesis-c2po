\chapter{Evaluation}


The \cpo\ analysis was implemented in \goblint.
In the following, we will denote as \base\ the \goblint\ analyzer without \cpo.
We evaluated the precision gain and the performance loss of the \cpo\ analysis, compared to \base, which includes a non-relational pointer analysis.
Moreover, we analyzed the difference in precision and performance between four different
configurations, which differ in the choice of the join algorithm and the choice of the equal function.
The four configurations are called:
\begin{enumerate}
\item \cpou\ uses the precise join with the automaton, and computes a canonical normal form for each domain element in order to compare them.
\item \cpod\ uses the join which considers the partition and computes \emph{equal} again by using the normal form.
\item \cpot\ uses the precise join with the automaton, and compares the equivalence classes of the partitions.
\item \cpoq\ uses the join which considers the partition and computes \emph{equal} by comparing the equivalence classes.
\end{enumerate}
A domain of the analyzer that tracks equalities but was found to make optimistic assumptions that do not hold in general had to be disabled.
All benchmarks were conducted on a machine with two Intel Xeon Platinum 8260 CPUs and 512 GB of RAM.

\section{Precision}

The precision of the \cpo\ analysis does not depend on the choice of the algorithm for \emph{equal}, as the two possible implementations produce the same results.
Therefore, we will only compare the precision of the \cpou, \cpod\ and \base\ analysis in the following.


\section{Efficiency (SV-Comp)}


We devised 29 \emph{litmus} tests inspired by real-world programming patterns,
designed to incorporate complex manipulations of pointers.
Each test was annotated with assertions to be verified.
While \cpo\ was able to prove all assertions,
only 16\% of the assertions could be proven by \base.
These programs may serve as benchmarks for future analyzers,
and we plan to contribute them to SV-COMP.

We further evaluated the precision of \cpo\ on 11 GNU core utilities programs (Coreutils).
We used \cpo\ to identify program invariants and generate assertions.
\ignore{as these programs are too extensive for manual annotation.}
For two of these programs, the generation of invariants timed out after one hour. % of runtime.
For the remaining nine programs, the \base\ analysis could prove only 24\% of the generated invariants.
In contrast, \cpo\ was able to prove all invariants.
%
Table~\ref{tab:summary} summarizes these results.

\begin{table}[t]
    \centering
    \caption{Summary of precision experiments. For each group of programs, the number of programs, the lines of code and the total number of invariants generated by \cpo\ are given.
        \ding{51} indicates that all assertions are proven, otherwise the number of proven assertions is given.}
    \label{tab:summary}
    % must use \input instead of \include to compile
    \input{content/eval-table}
\end{table}

For evaluating the performance, we executed \cpo\ on the reachability set
of the SV-COMP 2024 benchmarks~\cite{Beyer24} and compared its performance against \base.
Each task was given a timeout of 900 seconds and a memory limit of 15 GB.\@
We used the configuration of the analyzer for SV-COMP but removed the optimistic equality analysis.
% mentioned before.
We ran the analyzer with and without \cpo.
We observed that the analysis with \cpo\ has a 95th percentile slowdown of factor 2.93 compared to the base setting,
indicating that 95\% of tests in the benchmark take at most around three times as long as the base setup.
The median slowdown is 1.09.
This demonstrates that our analysis scales reasonably well.

\ignore{The runtime difference of the two configurations varied significantly between the benchmarks.
In some cases, \base\ timed out while \cpo\ proved the property in less than 30 seconds.
However, in most cases, \cpo\ incurred a noticeable performance overhead compared to \base.
Given the wide variance in performance, the mean slowdown is not a significant indicator.
Therefore, }

The reachability set of the SV-COMP benchmarks contains over 20,000 tests
where reachability of an error function is to be decided.
The analysis with \cpo\ infers extra properties, which do not necessarily improve the verdict,
as finding the correct verdict may require intricate arithmetic reasoning, which is orthogonal to our goal here.
For this reason, our analysis only succeeds in proving the verdict for 27 SV-COMP tests that \base\ could not prove.
Due to the computational overhead, 62 of the tests that could be proven by \base\ timed out with \cpo,
out of more than 2000 passed verdicts.

% Our implementation as well as the litmus tests and evaluation results will be made publicly available upon acceptance.
