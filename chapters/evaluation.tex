\chapter{Evaluation}\label{chapter:evaluation}


The \cpo\ analysis was implemented in the \goblint\ static analyzer.
\goblint\ is a static analyzer for C programs that is based on abstract interpretation
that already contains a set of basic abstract domains and analyses,
including interval analysis, inter-procedural analyses, and a non-relational pointer analysis.
A \goblint\ analysis that tracks equalities but was found to make optimistic assumptions that do not hold in general had to be disabled.
In the following, we will denote as \base\ the basic \goblint\ analyzer without \cpo.
We evaluated the precision gain and the performance loss of the \cpo\ analysis and compared it to \base.
Moreover, we analyzed the difference in precision and performance between four different
configurations, which differ in the choice of the join algorithm and the choice of the equal function.
The four configurations are denoted as:
\begin{enumerate}
\item \cpou\, which uses the precise join with the automaton, and computes a canonical normal form for each domain element in order to compare them.
\item \cpod\, which uses the join that considers the partition and computes \emph{equal} again by using the normal form.
\item \cpot\, which uses the precise join with the automaton, and compares the equivalence classes of the partitions.
\item \cpoq\, which uses the join which considers the partition and computes \emph{equal} by comparing the equivalence classes.
\end{enumerate}
All benchmarks were conducted on a machine with two Intel Xeon Platinum 8260 CPUs and 512 GB of RAM.

\section{Precision}

The precision of the \cpo\ analysis does not depend on the choice of the algorithm for \emph{equal}, as the two possible implementations produce the same results.
Therefore, we will only compare the precision of the \cpou, \cpod\ and \base\ analysis in the following.
They wee evaluated on two test suites.

The first suite consists of 29 \emph{litmus} tests inspired by real-world programming patterns,
designed to incorporate complex manipulations of pointers.
Each test was annotated with assertions to be verified.
\cpou\ was able to prove all assertions,
while only 16\% of the assertions could be proven by \base.
\cpod\ was able to prove all assertions except one, which was the example of TODO.\todo{write example and cite it here}
These programs may serve as benchmarks for future analyzers.

We further evaluated the precision of \cpo\ on 11 GNU core utilities programs (Coreutils).
We used \cpou\ to identify program invariants and automatically generate assertions.
For one of these programs, the generation of invariants timed out after one hour.
For the remaining nine programs, the \base\ analysis could prove only 24\% of the generated invariants.
In contrast, \cpou\ was able to prove all invariants.
\cpod\ could prove more than 99\% of the invariants.

Table~\ref{tab:summary} summarizes these results.

\begin{table}[t]
    \centering
    \caption{Summary of precision experiments. For each group of programs, the number of programs, the lines of code and the total number of invariants generated by \cpou\ are given.
        \ding{51} indicates that all assertions are proven, otherwise the number of proven assertions is given.}
    \label{tab:summary}
    % must use \input instead of \include to compile
    \input{content/eval-table}
\end{table}

We can conclude that the \cpo\ analysis is able to prove many assertions about relations between pointers that are out of reach for the \base\ analysis.
The \cpou\ analysis is, as expected, more precise than the \cpod\ analysis, as its join operation maintains more equalities in the case that the quantitative automaton contains cycles.
However, the difference in precision is minimal, indicating that in practice, the cases where there is a cycle in the automaton do not occur frequently.
Therefore, the choice of the join operation does not significantly affect the precision of the analysis.

\section{Efficiency}

For evaluating the performance, we executed \cpo\ on the reachability set
of the SV-COMP 2024 benchmarks~\cite{Beyer24} and compared the performance of the four possible configurations against \base.
Each task was given a timeout of 300\todo{do the evaluation with 900 seconds but only on unreach call} seconds and a memory limit of 15 GB.\@
We used the configuration of the analyzer for SV-COMP but removed the optimistic equality analysis mentioned before.
We ran the analyzer with each of the four configurations and once without \cpo.

\begin{table}[t]
    \centering
    \caption{Summary of efficiency experiments on the reachability set
    of the SV-COMP 2024 benchmarks. TODO}
    \label{tab:summary-svcomp}
    % must use \input instead of \include to compile
    \input{content/eval-table-2}
\end{table}\todo{caption}

We observed that the analysis with \cpo\ has a 95th percentile slowdown of at most a factor 2.61 compared to the base setting,
indicating that 95\% of tests in the benchmark take at most around three times as long as the base setup.
The median slowdown is 1.07.
This demonstrates that our analysis scales reasonably well.
There was no significant difference in performance between the four configurations.

The runtime difference of the two configurations varied significantly between the benchmarks.
In some cases, \base\ timed out while \cpo\ proved the property in less than 30 seconds.
However, in most cases, \cpo\ incurred a noticeable performance overhead compared to \base.
Given the wide variance in performance, the mean slowdown is not a significant indicator.

The reachability set of the SV-COMP benchmarks contains over 20,000 tests
where reachability of an error function is to be decided.
The analysis with \cpo\ infers extra properties, which do not necessarily improve the verdict,
as finding the correct verdict may require intricate arithmetic reasoning, which is orthogonal to our goal here.
For this reason, our analysis only succeeds in proving the verdict for 17 SV-COMP tests that \base\ could not prove.
Due to the computational overhead, approximately 120 of the tests that could be proven by \base\ timed out with \cpo,
out of more than 2000 passed verdicts.

The results are summarized in \cref{tab:summary-svcomp}.

Given the negligible performance differences observed across the four configurations, it is reasonable to conclude that these configurations are equivalent in practice, both in terms of performance and precision.
Therefore, the most practical choice may be the configuration that is simplest to implement---specifically, the one that avoids computing the normal form and employs the less precise join operation (\cpoq).

% Our implementation as well as the litmus tests and evaluation results will be made publicly available upon acceptance.
